{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1GQDizuU1xK","executionInfo":{"status":"ok","timestamp":1682005497873,"user_tz":-600,"elapsed":13361,"user":{"displayName":"Yuyun LIU","userId":"05887418251365522134"}},"outputId":"3de5ea7c-4cb8-45a6-cc28-0951febb5857"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# This mounts your google drive so we can have access to the data. To access\n","# the shared folder \"dalyell-2023\", you have to right click on the folder\n","# and add it as a shortcut in your own google drive. Only needs to be run once.\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2izBpZ4_U1xP"},"outputs":[],"source":["import pickle\n","import pandas as pd\n","import numpy as np\n","import argparse\n","\n","import data_collection_and_cleaning as dc\n","\n","import sklearn\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.neural_network import MLPClassifier\n","\n","# The function to start the script, including the whole process of the workflow.\n","def start(mode='normal'):\n","    print(\"Welcome user our personalized Twitter feeds CLI! Please follow the following steps to start the program!\")\n","\n","    # Get inputs of user ids, category and maximum number of tweets to view\n","    ids = input(\"Please enter user ids: \").split()\n","    category = input(\"Please enter the category you prefer: \")\n","    max_num_tweets_to_view = int(input(\"Please enter the number of tweets you're willing to view: \"))\n","\n","    # Initialize the dataset (tweets) and model\n","    tweets, model = init(category, ids, mode)\n","    tweets = tweets_classification(model, tweets)\n","\n","    # Workflow: push the tweet and receive feedback\n","    for i in range(0, max_num_tweets_to_view):\n","        tweets, model = push_tweet_and_get_input(model, tweets)\n","\n","    # Retrain the model again\n","    model = retrain(model, tweets)\n","    tweets = tweets_classification(model, tweets)\n","    \n","    # Save the labelled tweets and the trained model\n","    tweets.to_csv(\"datasets/\" + category + \"_labelled.csv\")\n","    pickle.dump(model, open(\"models/\" + category + \".sav\", 'wb'))\n","\n","    print(\"End train....\")\n","\n","# The function for program initialization\n","def init(category, ids, mode='normal'):\n","    print(\"Start collect tweets from the entered users... ...\")\n","\n","    # Init according to the mode selected. \n","    # A normal mode is for normal data gathering and cleaning.\n","    # The stub mode is for loading cleaned data for testing. (Avoid call Tweepy too often.)\n","    if mode != 'normal':\n","        df = collect_and_clean_tweets(category, ids)\n","    else:\n","        # stub\n","        df = collect_and_clean_tweets_stub(category)\n","    \n","    # Init database and model\n","    df = init_database(df)\n","    model = init_model(df)\n","\n","    return df, model\n","\n","# Tweets collection and cleaning.\n","def collect_and_clean_tweets(category, ids):\n","    dc.collect_tweets(category, ids)\n","    df = dc.clean_tweets(category)\n","    return df\n","\n","# Tweets collection and cleaning. (Stub)\n","def collect_and_clean_tweets_stub(category):\n","    # direct load from datasets\n","    df = pd.read_csv(\"datasets/\" + category + \".csv\")\n","    return df\n","\n","# Init database (dataframe)\n","def init_database(df):\n","    df[\"pred\"] = np.random.randint(0,2, size=len(df))\n","    df[\"label\"] = None\n","    return df\n","\n","# Init model for text classification.\n","def init_model(df):\n","    # create an empty model\n","    text_clf = Pipeline([\n","     ('vect', CountVectorizer()),   # converts text into feature vectors (see bag-of-words)\n","     ('tfidf', TfidfTransformer()), # converts to frequencies + applying weighting to certain tokens (see tf-idf)\n","     ('clf', MLPClassifier(hidden_layer_sizes=(10), activation='relu', solver='lbfgs', random_state=42, max_iter=200)),      # model, we can replace this with whatever model we want\n","    ])\n","    \n","    # Train with init dataset df\n","    x = df[\"text\"]\n","    y = df[\"pred\"]\n","\n","    text_clf.fit(x, y)\n","\n","    return text_clf\n","\n","# The function for tweets classification according to the current model\n","def tweets_classification(model, tweets):\n","    # classify tweets based on the current model\n","    predicts = model.predict(tweets[\"text\"])\n","    tweets[\"pred\"] = predicts\n","\n","    return tweets\n","\n","def push_tweet_and_get_input(model, tweets):\n","    # push the first tweets unlabelled and predicted as 0\n","    unlabelled_tweets = tweets[tweets['label'].isnull()]\n","    unlabelled_tweets_0 = unlabelled_tweets[unlabelled_tweets['pred'] == 0]\n","\n","    push_tweet = unlabelled_tweets_0.iloc[0]\n","    print(\"\\n*******************************\\n\" + push_tweet['content'] + \"\\n*******************************\")\n","    print(\"Please select from the following options:\\n\\t1. Confirm\\n\\t2. Wrong\\n\\t3. Retrain\")\n","    selection = int(input(\"Your selection: \"))\n","    \n","    if selection == 1:\n","        tweets = confirm(tweets, push_tweet)\n","    elif selection == 2:\n","        tweets = wrong(tweets, push_tweet)\n","    elif selection == 3:\n","        model = retrain(model, tweets)\n","        tweets = tweets_classification(model, tweets)\n","    else:\n","        print(\"Please select from option 1, 2 and 3.\")\n","    \n","    return tweets, model\n","\n","# Option 1\n","def confirm(tweets, push_tweet):\n","    target_id = push_tweet['id']\n","    tweets.loc[tweets['id'] == target_id,'label'] = 0\n","    \n","    return tweets\n","\n","# Option 2\n","def wrong(tweets, push_tweet):\n","    target_id = push_tweet['id']\n","    tweets.loc[tweets['id'] == target_id,'label'] = 1\n","\n","    return tweets\n","\n","# Option 3\n","def retrain(model, tweets):\n","    print(\"Retrain the model... ...\")\n","    tweets_labelled = tweets[~tweets['label'].isnull()]\n","    x = tweets_labelled[\"text\"]\n","    y = tweets_labelled[\"label\"]\n","    y=y.astype('int')\n","\n","    model.fit(x, y)\n","    return model\n","\n","if __name__ == \"__main__\":\n","    # Create the parser\n","    parser = argparse.ArgumentParser()\n","\n","    # Add an argument\n","    parser.add_argument(\"-m\", \"--mode\", action='store_true')\n","\n","    # Parse the argument\n","    args = parser.parse_args()\n","\n","    mode = None\n","    if args.mode == True:\n","        mode = 'stub'\n","    else:\n","        mode = 'normal'\n","\n","    # Start the script\n","    start(mode)"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}